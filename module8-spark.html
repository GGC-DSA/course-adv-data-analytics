<!doctype html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

  <title>Module 8 - Big Data algorithms and tools: Spark - ITEC 4220 Advanced Data Analytics</title>

  <link rel="stylesheet" href="css/reveal.css">
  <link rel="stylesheet" href="css/theme/black.css">

  <!-- Theme used for syntax highlighting of code -->
  <link rel="stylesheet" href="lib/css/zenburn.css">

  <!-- Printing and PDF exports -->
  <script>
  var link = document.createElement( 'link' );
  link.rel = 'stylesheet';
  link.type = 'text/css';
  link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
  document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section data-markdown>
          <textarea data-template>
            <img src="images/Spark_cover.png" style="float: right; max-width: auto; height: 500px; margin: 1em;"/>
              ## ITEC 4220 - Advanced Data Analytics

              ### Module 8 - Big Data algorithms and tools: Spark

              #### Cengiz Gunay, Rick Price, Fall 2020
              #### Reading: Ch 2 Spark - The definitive guide
          </textarea>
        </section>
        <section> <!-- Begin vertical for Hadoop Streaming -->
          <section data-markdown>
            <textarea data-template>
              ### Python example for Hadoop streaming (Mapper)
              - Need to run `chmod a+x map.py` to make it executable
              ```python
              #! /usr/bin/python
              # Map function
              import sys
              for line in sys.stdin:
                val = line.strip()
                (date, temp) = (val[10:14], val[15:20])
                print ("%s\t%s" % (date, temp))
              ```
            </textarea>
          </section>
          <section data-markdown>
            <textarea data-template>
              ### Python example for Hadoop streaming (Reducer)
              - Need to run `chmod a+x reduce.py` to make it executable

              ```python
              #! /usr/bin/python
              # Reduce function
              import sys
              (last_key, max_val) = (None, -99999)
              for line in sys.stdin:
                (key, val) = line.strip().split("\t")
                if last_key and last_key != key:
                  print ("%s\t%s" % (last_key, max_val))
                  (last_key, max_val) = (key, int(float(val)))
                else:
                  (last_key, max_val) = (key, max(max_val, int(float(val))))
                  if last_key:
                    print ("%s\t%s" % (last_key, max_val))
              ```
            </textarea>
          </section>
          <section data-markdown>
            <textarea data-template>
              ### How to run it

              On the command line without Hadoop:
              ```bash
              % cat Daily.txt | python map.py | sort | python reduce.py
              0727	0
              0728	92
              0729	90
              0730	91
              0731	106
              0801	108
              ```
            </textarea>
          </section>
          <section data-markdown>
            <textarea data-template>
              ### With Hadoop

              ```bash
              % bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
                -input input/Daily.txt \
                -output output \
                -mapper map.py \
                -reducer reduce.py
              ```
              ```bash
              0727    0
              0728    92
              0729    90
              0730    91
              0731    106
              0801    108
              ```
            </textarea>
          </section>
        </section>
        <section data-markdown>
          <textarea data-template>
            ### What comes after Hadoop?

            - Hadoop created an ecosystem of projects:
              - Avro: Data serialization system
              - Flume: Work with data streams
              - Sqoop: Interface with traditional relational DBs
              - Hive: SQL queries converted to MapReduce
              - [Pig](https://www.slideshare.net/kevinweil/hadoop-pig-and-twitter-nosql-east-2009/): Hadoop processing with custom high-level language
              - Parquet: Columnar storage for nested data
              - Crunch: High-level API for using Hadoop
              - [Kafka](https://kafka.apache.org/): Distributed streaming platform
              - [Spark](https://spark.apache.org/): Another distributed computing framework
          </textarea>
        </section>
    <section> <!-- Begin vertical for Spark Explanation-->
          <section data-markdown>
            <textarea data-template>
              <img src="images/Spark_cover.png" style="float: right; max-width: auto; height: 500px;"/>
              ### Spark

              - Since 2009
              - Written in Scala running on JVM
            </textarea>
          </section>
          <section data-markdown>
            <textarea data-template>
              ### Features

              - Speed
                - Up to 100 times faster than Hadoop
                - Supports in-memory storage of intermediate results
              - Multiple languages supported
                - Python
                - Scala
                - R
                - Java
              - Advanced Analytics
                 - SQL queries
                 - Streaming data
                 - Machine learning
                 - Graph algorithms
            </textarea>
          </section>
          <section data-markdown>
            <textarea  data-template>
              ### Spark and Hadoop Differences

              Hadoop:
              - Hadoop is a batch process
              - Hadoop processes data from the HDFS and writes intermediate results to HDFS
              - Results are written to the HDFS

              Spark:
              - Can read data from streaming, local file system, HDFS or other distributed file system
            </textarea>
          </section>
          <section data-markdown>
            <textarea  data-template>
              ### Spark Components
              <img src="images/SparkComponents.png" style="float: right; max-width: auto; height: 500px;"/>
            </textarea>
          </section>
          <section data-markdown>
            <textarea  data-template>
              ### Resilient Distributed Datasets (RDD)
              - Fundamental data structure for Spark
                - Immutable distributed collection of objects
              - Divided into logical partitions
                - Can be computed on different nodes
                - Can contain any data type or Python, Java, or Scala
                - Can contain User Defined Types (UDTs)
            </textarea>
          </section>
          <section data-markdown>
            <textarea  data-template>
              ### Spark SQL
              - Integrates relational processing
              - Supports queries in SQL and HQL (Hive Query Language)
              - Supports various data sources
                - CSV, Text, JSON, etc.
              - Libraries
                - DataSource API
                - DataFrame API
                - Interpreter and Optimizer
                - SQL Service
            </textarea>
          </section>
          <section data-markdown>
            <textarea  data-template>
              ### Spark Streaming
              - Process real-time streaming data
              - Fundamental stream unit is DStream
                - Series of RDDs to process real-time data
              - Viewed as a continually growing table
            </textarea>
          </section>
          <section data-markdown>
            <textarea  data-template>
              ### MLib
              - Machine Learning Library
              - Tools
                - ML Algorithms
                  - Classification, Regression, CLustiner, Collaborative Filtering
                - Featurization
                  - Feature extraction, transformations, and dimention reduction
              - Pipelines
                - Contruct and training ML pipelines
              - Persistence
                - Saving and loading algorithms, models and pipelines
              - Utilities
                - Linear algebra, statistics, data handling
            </textarea>
          </section>
          <section data-markdown>
            <textarea  data-template>
              ### GraphX
              - API for graphs and graph-parallel computation
              - Extends RDD with a Resilient Distributed Property Graph
              - Can have parallel edges to define multiple relationships between vertices
            </textarea>
          </section>
          <section data-markdown>
            <textarea  data-template>
              ### GraphFrames
              - Similar to GraphX but built on top of DataFrames
              - DataSet
                - Distributed collection of data
                - Strong typing
                - Available in Scala and Java
              - DataFrame
                - DataSet organized into named columns
                - Representation of DataSet in rows
                - Available in Scala, Java, Python and R
            </textarea>
          </section>
        </section> <!-- end Spark Explanation -->
        <section> <!-- Spark coding -->
          <section data-markdown>
            <textarea data-template>
              ### Spark practice and follow-along

              - Log into your VM
              - `cd /home/spark`
              - Run commands by prefixing with `bin/` (such as `bin/pyspark` or `bin/spark-submit`)
            </textarea>
          </section>
          <section data-markdown>
          <textarea data-template>
            ### Spark works with _resilient distributed datasets_ (RDDs)

            - To use Python for Spark, run: `pyspark`
            - A line count program example:
            ```python
            >>> lines = sc.textFile("README.md") # Create an RDD called lines
            >>> lines.count() # Count the number of items in this RDD
            127 # May vary based on your version of Spark
            >>> lines.first() # First item in this RDD, i.e. first line of README.md
            u'# Apache Spark'
            ```
            - Parallel operation completely transparent!
            - `sc` is the `SparkContext` _driver_ to access Spark
              and create RDDs
            - RDD is broken into pieces to run `count()` in parallel
          </textarea>
         </section>
         <section data-markdown>
          <textarea data-template>
            ### Inside Spark

            <img src="images/Spark_Ch2_fig2-3_components.png" style="max-width: auto; height: 300px;"/>
            - Another example with custom filter function: <!-- .element: class="fragment" data-fragment-index="2" -->

```python
>>> lines = sc.textFile("README.md")
>>> pythonLines = lines.filter(lambda line: "Python" in line)
>>> pythonLines.first()
u'## Interactive Python Shell'
```
        </textarea>
       </section>
       <section data-markdown>
        <textarea data-template>
          ### Working with RDDs

          - Transformations_ create new RDDs; e.g.:

```python
>>> pythonLines = lines.filter(lambda line: "Python" in line)
```
          - Actions_ calculate results from RDDs; e.g.:

```python
>>> pythonLines.first()
```

        </textarea>
       </section>
       <section data-markdown>
        <textarea data-template>
         ### RDD transformations

         - Most commonly used: `map()` and `filter()`
         <img src="images/Spark_Ch3_fig3-2_map-filter.png" style="max-width: auto; height: 300px;"/>
```python
mapex = rdd.map(lambda x: x * x)
```
         - If multiple outputs for each input, then use `flatMap()`
         - Set operations: `distinct()`, `union()`, `intersection()`, `subtract()`, `cartesian()` (expensive)
        </textarea>
       </section>
       <section data-markdown>
        <textarea data-template>
         ### RDD actions

         - Most common `reduce()`: Takes two inputs and outputs one
```python
sum = rdd.reduce(lambda x, y: x + y)
```
         - `fold()` also asks a _zero_ value for initialization
```python
sum = rdd.fold(0, lambda x, y: x + y)
```
         - `aggregate()` asks for accummulation and combine functions. Example that calculates a running sum and count of elements to calculate an average value:
```python
sumCount = nums.aggregate((0, 0),
    (lambda acc, value: (acc[0] + value, acc[1] + 1)),
    (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])))
    return sumCount[0] / float(sumCount[1])
```
        </textarea>
       </section>
      <section data-markdown>
       <textarea data-template>
        ### RDD actions (continued)

        - `collect()`: Return all elements
        - `count()`: Number of elements
        - `countByValue()`: Number of times each element occurs in RDD (histogram)
        - and more...
       </textarea>
      </section>
        </section> <!-- end spark coding -->
        <section> <!-- SQL and DataFrame -->
          <section data-markdown>
            <textarea data-template>
        ### Spark SQL

        - Uses _DataFrame_ RDD objects that hold semi-structured data
        - Can read/write data in JSON, Hive Tables, and Parquet
        - SQL interface to be used inside or outside Spark (e.g. JDBC connection or from Tableau)
```python
hiveCtx = HiveContext(sc)
input = hiveCtx.jsonFile(inputFile)
# Register the input DataFrame
input.registerTempTable("tweets")
# Select tweets based on the retweetCount
topTweets = hiveCtx.sql("""SELECT text, retweetCount FROM tweets
  ORDER BY retweetCount LIMIT 10""")
```
            </textarea>
          </section>
          <section data-markdown>
            <textarea data-template>
              ### Spark Dataframes provide flexible quering options

              Spark supports [DataFrames](https://spark.apache.org/docs/latest/sql-getting-started.html#creating-dataframes) natively:
              
              ```python
# load CSV file as DataFrame object in Spark 
# (read the first row as column headers)
df = spark.read.csv("SamGroom-assign6/athlete_events.csv", header = True)
# show info
df.printSchema()
df.show()
# run queries
males = df.filter(df.Sex == "M")
malesYears = males.filter(males.Year > 1996).filter(males.Year < 2006)
              ```
            </textarea>
          </section>
          <section data-markdown>
            <textarea data-template>
              ### Also can convert from and to Pandas Dataframes

              - [Pandas](https://pandas.pydata.org/) provides the standard Dataframe object in Python
              - Enable Apache Arrow in Spark [to use Pandas objects](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html)              

              ```python
import numpy as np
import pandas as pd

# Enable Arrow-based columnar data transfers
spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")

# Generate a Pandas DataFrame
pdf = pd.DataFrame(np.random.rand(100, 3))

# Create a Spark DataFrame from a Pandas DataFrame using Arrow
df = spark.createDataFrame(pdf)

# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow
result_pdf = df.select("*").toPandas()
              ```
            </textarea>
          </section>
        </section> <!-- end SQL and DataFrame -->
     <section data-markdown>
      <textarea data-template>
       ### Closing notes for Spark
       <img src="images/Spark_cover.png" style="float: right; max-width: auto; height: 500px;"/>

       Spark also provides:
        - [Real-time processing via streaming](https://spark.apache.org/docs/latest/streaming-programming-guide.html) (Chapter 10)
        - [Machine learning library, _MLlib_](https://spark.apache.org/docs/latest/ml-guide.html) (Chapter 11)

        More resources:
         - [Book's Github repo with examples](https://github.com/databricks/learning-spark)
         - [Spark 2.4.4 API documentation](https://spark.apache.org/docs/2.4.4/) - <br>
            JetStream image version
         - [Spark Quick Start](https://spark.apache.org/docs/latest/quick-start.html)
         - [RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)
      </textarea>
     </section>

      </div>
    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>
      // More info about config & dependencies:
      // - https://github.com/hakimel/reveal.js#configuration
      // - https://github.com/hakimel/reveal.js#dependencies
      Reveal.initialize({
        math: {
  mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js', // https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js
  config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
        },

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1280, // 960
        height: 720, // 700

        // Factor of the display size that should remain empty around the content
        margin: 0.1,
        history: true,

        // Bounds for smallest/largest possible scale to apply to content
        minScale: 0.2,
        maxScale: 1.5,

        dependencies: [
  { src: 'plugin/markdown/marked.js' },
  { src: 'plugin/markdown/markdown.js' },
  { src: 'plugin/notes/notes.js', async: true },
  { src: 'plugin/highlight/highlight.js',
    async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
                // MathJax
  { src: 'plugin/math/math.js', async: true }
        ]
      });
    </script>
  </body>
</html>
